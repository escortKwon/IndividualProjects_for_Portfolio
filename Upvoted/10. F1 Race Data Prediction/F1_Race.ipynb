{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Outline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve, plot_roc_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Unzip archive\n",
    "!Unzip -o archive.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge raw datas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create empty DataFrame availbe for accpet values\n",
    "# F1_data = pd.DataFrame()\n",
    "# F1_data_year = pd.DataFrame()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utility Function\n",
    "# def get_DataFrame_F1(dirname):\n",
    "    #global F1_data\n",
    "    #global F1_data_year\n",
    "    #foldernames = os.listdir(dirname)\n",
    "    #for folder in foldernames:\n",
    "        #foldername = os.path.join(dirname, folder)\n",
    "        #for file in foldername:\n",
    "            #filenames = os.listdir(foldername)\n",
    "            #for file in filenames:\n",
    "                #filename = os.path.join(foldername, file)\n",
    "                #F1_data_temp = pd.read_csv(filename)\n",
    "                #F1_data_year = pd.concat([F1_data_year, F1_data_temp])\n",
    "    #F1_data = pd.concat([F1_data, F1_data_year])\n",
    "    #return F1_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define dirname and create full DataFrame for EDA\n",
    "# dirname = 'race_wise_data'\n",
    "# get_DataFrame_F1(dirname)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load merged data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "F1_data = pd.read_csv('/Users/escortkwon/Code/MiniProjects_Kaggle/Upvoted/10. F1 Race Data Prediction/final.csv')\n",
    "F1_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check features 'F1_data' contains\n",
    "print('Features: ', F1_data.columns)\n",
    "F1_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check whether 'crimes' contains any Null or NaN values\n",
    "F1_data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Additional arranging 'F1_data'\n",
    "F1_data.sort_values(by=['season', 'round'], ascending=True, inplace=True)\n",
    "F1_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "F1_data.reset_index(drop=True, inplace=True)\n",
    "F1_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create another DataFrame for training\n",
    "F1_data_train = F1_data.copy()\n",
    "F1_data_train.drop(['season', 'round', 'circuit_id', 'driver', 'nationality', 'constructor'], axis=1, inplace=True)\n",
    "\n",
    "weathers = ['weather_warm', 'weather_cold', 'weather_dry', 'weather_wet', 'weather_cloudy']\n",
    "for weather in weathers:\n",
    "    F1_data_train[weather] = F1_data_train[weather].apply(lambda x : 1 if x == True else 0)\n",
    "\n",
    "F1_data_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "The dataset has already been preprocessed, so there're nothing left to do.  \n",
    "But, it contains 21 features which are relatively more than any other datasets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decomposition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check corrleation of 'F1_data'\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('Corrleation Heatmap of F1_data')\n",
    "sns.heatmap(F1_data.corr(), annot=True, fmt='.1g', linewidths=.3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "As you can see heatmap above, featrues from 'grid' to 'constructor_standing_pos' are heavily related with dataset.  \n",
    "Therefore, using PCA, we are gonna process decomposition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_dec = ['grid', 'podium', 'driver_points', 'driver_standings_pos', 'constructor_points', 'constructor_wins', 'constructor_standings_pos']\n",
    "F1_data_scaled = F1_data.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "F1_data_scaled = scaler.fit_transform(F1_data[features_dec])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(F1_data_scaled)\n",
    "print('Variability by PCA Components: ', pca.explained_variance_ratio_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "We can explain explain the variance of 7 features with 2 PCA components  \n",
    "The total variance is about 73% and the first axis was the highest with 55%"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set X, y as features and label\n",
    "F1_data_scaled = F1_data_train.copy()\n",
    "\n",
    "X_features = F1_data_scaled.drop('driver_wins', axis=1, inplace=False)\n",
    "y_label = F1_data_scaled['driver_wins']\n",
    "print('Shape of X_features: {0} / Shape of y_label: {1}'.format(X_features.shape, y_label.shape))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate accuracy score without PCA\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, random_state=11)\n",
    "scores = cross_val_score(rf_clf, X_features, y_label, scoring='accuracy', cv=3)\n",
    "\n",
    "print('Accuracy by each fold: ', scores)\n",
    "print('Average Accuracy: {0:.4f}'.format(np.mean(scores)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate accuracy score with PCA\n",
    "pca = PCA(n_components=7)\n",
    "df_pca = pca.fit_transform(X_features)\n",
    "scores_pca = cross_val_score(rf_clf, df_pca, y_label, scoring='accuracy', cv=3)\n",
    "\n",
    "print('Accuracy by each fold PCA converted: ', scores_pca)\n",
    "print('Average Accuracy PCA converted: {0:.4f}'.format(np.mean(scores_pca)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "The number of columns was decreased from 14 to 7, so the decreased rate is 50%  \n",
    "But, the accuracy score was decreased from 88.99% to 83.44%, which means 6% decreased  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution of Weather"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract data for plotting distribution\n",
    "F1_data_weather = F1_data.iloc[:, [3, 4, 5, 6, 7]]\n",
    "\n",
    "# Check distribution of each features\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(F1_data_weather.columns):\n",
    "    row = int(i/3)\n",
    "    col = i%3\n",
    "    sns.distplot(F1_data_weather.iloc[:, i], ax=axs[row][col])\n",
    "\n",
    "plt.suptitle('Distirbution of Weather')\n",
    "plt.tight_layout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataframes for pie charts\n",
    "F1_data_weather_binary = F1_data_train.iloc[:, range(0, 5)]\n",
    "F1_weather_warm = pd.value_counts(F1_data_weather_binary['weather_warm'])\n",
    "F1_weather_cold = pd.value_counts(F1_data_weather_binary['weather_cold'])\n",
    "F1_weather_dry = pd.value_counts(F1_data_weather_binary['weather_dry'])\n",
    "F1_weather_wet = pd.value_counts(F1_data_weather_binary['weather_wet'])\n",
    "F1_weather_cloudy = pd.value_counts(F1_data_weather_binary['weather_cloudy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(24, 3))\n",
    "\n",
    "F1_weather_warm.plot.pie(ax=axs[0])\n",
    "F1_weather_cold.plot.pie(ax=axs[1])\n",
    "F1_weather_dry.plot.pie(ax=axs[2])\n",
    "F1_weather_wet.plot.pie(ax=axs[3])\n",
    "F1_weather_cloudy.plot.pie(ax=axs[4])\n",
    "\n",
    "plt.suptitle('Distribution of Weather by Pie Chart')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "That plot shows the distribution of weather.  \n",
    "Generally, it was warm and moderate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution of Nationality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create DataFrame for Pie Chart\n",
    "F1_data_nation = pd.DataFrame(data=F1_data['nationality'].value_counts())\n",
    "F1_data_nation.reset_index(inplace=True)\n",
    "F1_data_nation.rename({'index' : 'nationality', 'nationality' : 'count'}, axis=1, inplace=True)\n",
    "F1_data_nation.sort_values(by='count', ascending=False, inplace=True)\n",
    "F1_data_nation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encode features\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(F1_data_nation['nationality'])\n",
    "F1_data_nation['nationality_le'] = le.transform(F1_data_nation['nationality'])\n",
    "F1_data_nation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pie Chart\n",
    "pie, ax = plt.subplots(figsize=[10, 10])\n",
    "labels = F1_data_nation['nationality']\n",
    "plt.pie(F1_data_nation['count'], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\n",
    "plt.title(\"Distribution of Nationality\", fontsize=14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pie Chart\n",
    "pie, ax = plt.subplots(figsize=[10, 10])\n",
    "labels = F1_data_nation.iloc[0:10, 0]\n",
    "plt.pie(F1_data_nation.iloc[0:10, 1], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\n",
    "plt.title(\"Distribution of Nationality [Top 10]\", fontsize=14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "Usually, countries in Europe were superior to other countries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution of Age"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot density plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Distribution of Age')\n",
    "sns.distplot(F1_data['driver_age'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "The distribution of age of drivers were much similiar as Normal Distribution which is compatible for training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scaling\n",
    "for feature in F1_data_train.columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(np.array(F1_data_train[feature]).reshape(-1, 1))\n",
    "    F1_data_train[feature] = scaler.transform(np.array(F1_data_train[feature]).reshape(-1, 1))\n",
    "\n",
    "F1_data_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set X, y for features and label\n",
    "X = F1_data_train.drop('driver_wins', axis=1, inplace=False)\n",
    "y = F1_data_train['driver_wins'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of X_test: ', X_test.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of y_test: ', y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utility Function\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='macro')\n",
    "    recall = recall_score(y_test, pred, average='macro')\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba, multi_class='ovr', average='macro')\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion)\n",
    "    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utility Function\n",
    "def get_clf_eval_edit(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='macro')\n",
    "    recall = recall_score(y_test, pred, average='macro')\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_test, pred_proba, multi_class='ovo', average='macro')\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion)\n",
    "    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Estimator CLass\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# Fitting\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Pred_Proba\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "get_clf_eval_edit(y_test, lr_pred, lr_pred_proba)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "In fact, estimators such as LogisticRegression or DecisionTreeClassifier is very fundamental but powerful on binary class problems.  \n",
    "Although I choosed for LogisticRegreesion, its accuracy and AUC are 94.27% and 99.19% each.  \n",
    "LogisticRegression is known as good estimator to use not only binary problems but also mutliclass problems.  \n",
    "\n",
    "But, in this dataset, there are no binary features.  \n",
    "Therefore, it might be good idea to binarize features with threshold values in custom.  \n",
    "I will try for an update soon.\n",
    "\n",
    "Thanks for reviewing my Notebook!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('Drill_DL_TF': conda)"
  },
  "interpreter": {
   "hash": "919cf3ae0e9712d87b63b1e10ff40154688c8cbaba6389820679510ec494ce74"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}