{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, confusion_matrix, f1_score, precision_recall_curve, roc_curve, plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, plot_importance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Unzip archive\n",
    "!unzip -o employee_promotion.csv.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load data\n",
    "data = pd.read_csv('employee_promotion.csv')\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Overview\n",
    "|        Column        |                                         Descriptions                                        |\n",
    "|:--------------------:|:-------------------------------------------------------------------------------------------:|\n",
    "| employee_id          | Unique ID for the employee                                                                  |\n",
    "| department           | Department of employee                                                                      |\n",
    "| region               | Region of employment(unordered)                                                             |\n",
    "| education            | Education level                                                                             |\n",
    "| gender               | Gender of Employee                                                                          |\n",
    "| recruitment_channel  | Channel of recruitment for employee                                                         |\n",
    "| no_of_trainings      | no of other trainings completed in the previous year on soft skills, technical skills, etc. |\n",
    "| age                  | Age of Employee                                                                             |\n",
    "| previous_year_rating | Employee Rating for the previous year                                                       |\n",
    "| length_of_service    | Length of service in years                                                                  |\n",
    "| awards_won           | if awards won during the previous year then 1 else 0                                        |\n",
    "| avg_training_score   | Average score in current training evaluations                                               |\n",
    "| is_promoted          | Recommended for promotion                                                                   |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check whether any columns contain NaN or Null values\n",
    "data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We have lots of data, specifically over 10000\n",
    "# Therefore, I decided to drop rows contain NaN or Null\n",
    "data.dropna(axis=0, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 'emplye_id' column is unnecessary so, I gonna drop for it\n",
    "data.drop(columns=['employee_id'], inplace=True)\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert 'gender' column into binary values\n",
    "data['gender_new'] = pd.Series()\n",
    "data.loc[data['gender'] == 'f', 'gender_new'] = 0\n",
    "data.loc[data['gender'] == 'm', 'gender_new'] = 1\n",
    "data = data.astype({'gender_new' : 'int'})\n",
    "data.drop(columns=['gender'], inplace=True)\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "Finally, we got preprocessed DataFrame named 'data'.  \n",
    "It has no NaN or Null values, therefore it is good to go for machine learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check distributions of features for training by pie charts\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(40, 20))\n",
    "\n",
    "for i, feature in enumerate(data.columns):\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    pd.value_counts(data.iloc[:, i]).plot.pie(autopct=\"%.1f%%\", ax=axs[row][col])\n",
    "\n",
    "plt.suptitle('Distribution of features')\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check distributions of features contain numbers by distplot\n",
    "columns = ['gender_new', 'no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'awards_won', 'avg_training_score', 'is_promoted']\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "for i, feature in enumerate(data[columns]):\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    sns.distplot(data[columns].iloc[:, i], ax=axs[row][col])\n",
    "\n",
    "plt.suptitle('Distirbution of features')\n",
    "plt.tight_layout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "As you can see above, none of columns have normal distribution which is proper for training  \n",
    "Therefore, we can try converting following columns for normal distribution: 'age', 'length_of_service' and 'avg_training_score' "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Log Transformation\n",
    "age_log = np.log1p(data['age'])\n",
    "service_log = np.log1p(data['length_of_service'])\n",
    "score_log = np.log1p(data['avg_training_score'])\n",
    "\n",
    "data.insert(6, 'age_log', age_log)\n",
    "data.insert(9, 'length_of_service_log', service_log)\n",
    "data.insert(12, 'avg_training_score_log', score_log)\n",
    "\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check distributions of log converted columns\n",
    "log_columns = ['age_log', 'length_of_service_log', 'avg_training_score_log']\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "sns.distplot(data['age_log'], ax=ax1)\n",
    "ax1.set_title('Distribution of age_log')\n",
    "sns.distplot(data['length_of_service_log'], ax=ax2)\n",
    "ax2.set_title('Distribution of length_of_service_log')\n",
    "sns.distplot(data['avg_training_score_log'], ax=ax3)\n",
    "ax3.set_title('Distribution of avg_training_score_log')\n",
    "\n",
    "plt.suptitle('Distribution of log converted features', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "'age_log' became nearly normal distributed but other features didn't  \n",
    "Log transformation is one of the most powerful strategies of preparing data for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Correlation of features')\n",
    "sns.heatmap(data.corr(), annot=True, linewidths=.5, cmap=\"YlGnBu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "As I anticipated, 'age', 'length of service' and 'avg_training_score' was highly correlated  \n",
    "So, maybe I can try decomposition for those features (To be Continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split Datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get One-Hot encoded DataFrame\n",
    "data_oh = pd.get_dummies(data)\n",
    "data_oh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scaling of features\n",
    "features = np.array(data_oh.columns).reshape(-1, 1)\n",
    "\n",
    "for feature in features:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_oh[feature])\n",
    "    data_oh[feature] = scaler.transform(data_oh[feature])\n",
    "\n",
    "data_oh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define features and label for training\n",
    "train_features = data_oh.drop(columns=['is_promoted'], inplace=False)\n",
    "train_label = data_oh['is_promoted'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.2, random_state=11)\n",
    "\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of X_test: ', X_test.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of y_test: ', y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Estimators"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utility Function\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba, average=\"macro\")\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion)\n",
    "    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Process fitting, prediction and evalution by Logistic Regression\n",
    "# Create Estimator CLass\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "lr_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Fitting\n",
    "dt_clf.fit(X_train, y_train)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Pred_Proba\n",
    "dt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "rf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "get_clf_eval(y_test, dt_pred, dt_pred_proba)\n",
    "get_clf_eval(y_test, lr_pred, lr_pred_proba)\n",
    "get_clf_eval(y_test, rf_pred, rf_pred_proba)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Precision-Recall curve\n",
    "plot_precision_recall_curve(dt_clf, X_test, y_test)\n",
    "plot_precision_recall_curve(lr_clf, X_test, y_test)\n",
    "plot_precision_recall_curve(rf_clf, X_test, y_test)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot ROC curve\n",
    "plot_roc_curve(dt_clf, X_test, y_test)\n",
    "plot_roc_curve(lr_clf, X_test, y_test)\n",
    "plot_roc_curve(rf_clf, X_test, y_test)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comment\n",
    "As you can see above, LogisticRegression and RandomForestClassifier are both basic models but really powerful"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightGBM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create estimator and process fitting, prediction and evaluation for model after applying SMOTE\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "lgbm_preds_over = lgbm_clf.predict(X_test)\n",
    "lgbm_pred_proba = lgbm_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_test, lgbm_preds_over, lgbm_pred_proba)\n",
    "plot_roc_curve(lgbm_clf, X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create estimator and process fitting, prediction and evaluation for model\n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=400, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=1200, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "preds = lgbm_wrapper.predict(X_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, preds, pred_proba)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(lgbm_wrapper, ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comment\n",
    "LightGBM is also another powerful model which runs by Boosting(Boot Strapping)  \n",
    "It is more lighter than GradientBoostingClassifier(literally)  \n",
    "Specifically, comparing to basic estimators, AUC score was the highest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stacking Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create individual ML model\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=11)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# Create the model which will be fitted by dataset Stacking processed\n",
    "lr_final = LogisticRegression(C=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fitting each models\n",
    "knn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict each models and predict them\n",
    "\n",
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy Score of KNN: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\n",
    "print('Accuracy Score of RandomForestClassifier: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
    "print('Accuracy Score of DeicisionTreeClassifier: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
    "print('Accuracy Score of AdaBoostClassifier: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine preds to one ndarray\n",
    "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
    "print(pred.shape)\n",
    "\n",
    "# Transponse 'pred' in order to convert as Feature\n",
    "pred = np.transpose(pred)\n",
    "print(pred.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit, Predict, Evaluate for final model\n",
    "lr_final.fit(pred, y_test)\n",
    "final = lr_final.predict(pred)\n",
    "\n",
    "print('Accuracy Score of Final Model: {0:.4f}'.format(accuracy_score(y_test, final)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "**Accuracy Score of Final Model: 0.9428**\n",
    "\n",
    "1. Through trraining basic estimators, LightGBM and ensemble models, I could try lots of estimators for this datset.  \n",
    "2. Especially, the label for training was binary (which is 0 or 1) so, the evaluation score seems much higher than those from multi-classification.  \n",
    "3. Also, ensemble and nearest-neighbor model became ditinguished, therefore, visualzing scatter plot could be another good way for analyzing data.\n",
    "\n",
    "Thanks for reviewing my notebooks  \n",
    "If you'd like it, please give me upvotes and leave comments  \n",
    "Any questions or comments are always welcome"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('Drill_ML': conda)"
  },
  "interpreter": {
   "hash": "25d115b65964a764bdbc68961c822c1fcde684856a5a250666e2763eac196c24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}